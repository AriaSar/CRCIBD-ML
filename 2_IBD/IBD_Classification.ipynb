{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path_1 = os.path.dirname(os.getcwd())\n",
    "path_2 = path_1 + '/' + 'CommonCodes/'\n",
    "sys.path.insert(0, path_2)\n",
    "\n",
    "import warnings\n",
    "import operator\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import HelperFunctions as helper\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_USE_FIRST_N_GENES_CONF = 40\n",
    "_USE_FIRST_N_GENES_ALL = 100 \n",
    "\n",
    "with open('./IBD_features.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "    selected_genes = content.split(',')\n",
    "\n",
    "selected_genes = selected_genes[:100000]\n",
    "\n",
    "counts = Counter(selected_genes)\n",
    "counts = dict(sorted(counts.items(), key=operator.itemgetter(1), reverse=True))\n",
    "counts = list(counts.keys())[:_USE_FIRST_N_GENES_ALL]\n",
    "selected_features = counts\n",
    "selected_features.append('label')\n",
    "\n",
    "train_datasets_names = ['GSE16879', 'GSE22619', 'GSE59071', 'GSE102133', 'GSE179285']\n",
    "test_datasets_names = ['GSE9452', 'GSE36807' ,'GSE37283', 'GSE4183','GSE48958','GSE92415']\n",
    "\n",
    "_test_datasets = []\n",
    "for d in test_datasets_names:\n",
    "    _test_datasets.append(pd.read_pickle(path_1+'/Datasets/'+d))\n",
    "\n",
    "_train_datasets = []\n",
    "for d in train_datasets_names:\n",
    "    _train_datasets.append(pd.read_pickle(path_1+'/Datasets/'+d))\n",
    "\n",
    "for i in range(len(_test_datasets)):\n",
    "    _test_datasets[i] = _test_datasets[i].groupby(_test_datasets[i].columns, axis=1).agg(np.mean)[selected_features]\n",
    "    _test_datasets[i].columns = np.arange(len(_test_datasets[i].columns))\n",
    "\n",
    "for i in range(len(_train_datasets)):\n",
    "    _train_datasets[i] = _train_datasets[i].groupby(_train_datasets[i].columns, axis=1).agg(np.mean)[selected_features]\n",
    "    _train_datasets[i].columns = np.arange(len(_train_datasets[i].columns))\n",
    "\n",
    "df_train = pd.concat(_train_datasets)\n",
    "\n",
    "df_train = helper.oversample(df_train).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x_train, y_train = helper.datasetXY_S(df_train)\n",
    "\n",
    "for i in range(len(_test_datasets)):\n",
    "    x, y = helper.datasetXY_S(_test_datasets[i])\n",
    "    x = pd.DataFrame(x.apply(helper.minmax_scaler, axis=1).values.tolist(), columns=x.columns)\n",
    "    scaled_df = helper.datasetXY_M(x, y)\n",
    "    _test_datasets[i] = scaled_df.copy()\n",
    "\n",
    "x_train = pd.DataFrame(x_train.apply(helper.minmax_scaler, axis=1).values.tolist(), columns=x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "accuracies = []\n",
    "confusions = []\n",
    "recall_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "n_genes = [1,10,20,30,40,50,60,70,80,90,100]\n",
    "\n",
    "for n in n_genes:\n",
    "    \n",
    "    x_train_f = x_train.iloc[:,:n]  \n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=300)\n",
    "    lr_model = LogisticRegression()\n",
    "    svm_model = CalibratedClassifierCV(SVC(kernel='poly'), cv=5)\n",
    "\n",
    "    rf_model.fit(x_train_f, y_train)\n",
    "    lr_model.fit(x_train_f, y_train)\n",
    "    svm_model.fit(x_train_f, y_train)\n",
    "\n",
    "\n",
    "    n_genes_accs = []\n",
    "    n_genes_f1 = []\n",
    "    n_genes_recall = []\n",
    "    n_genes_specificity = []\n",
    "    \n",
    "    for curr_test_set_index in range(len(_test_datasets)):\n",
    "\n",
    "        x_test, y_test = helper.datasetXY_S(_test_datasets[curr_test_set_index])\n",
    "        x_test_f = x_test.iloc[:,:n]\n",
    "        \n",
    "        rf_pred_proba = rf_model.predict_proba(x_test_f)\n",
    "        lr_pred_proba = lr_model.predict_proba(x_test_f)\n",
    "        svm_pred_proba = svm_model.predict_proba(x_test_f)\n",
    "\n",
    "        ensemble_pred_probs = np.mean([rf_pred_proba, lr_pred_proba, svm_pred_proba], axis=0)\n",
    "\n",
    "        ensemble_preds = helper.probToDummy(ensemble_pred_probs)[:,1]\n",
    "        ensemble_preds = [int(x) for x in ensemble_preds]\n",
    "\n",
    "        f1 = f1_score(y_test, ensemble_preds)\n",
    "        acc = accuracy_score(y_test, ensemble_preds)\n",
    "\n",
    "        n_genes_f1.append(f1)\n",
    "        n_genes_accs.append(acc)                    \n",
    "\n",
    "        if n == _USE_FIRST_N_GENES_CONF and y_test.value_counts().shape[0] == 2 :\n",
    "            precision, recall, average_precision = helper.precision_recall(y_test, ensemble_pred_probs)\n",
    "            classes = ['Control', 'Case']\n",
    "\n",
    "            zom = [0,1,'micro']\n",
    "            for z in zom:\n",
    "                recall[z] = np.insert(recall[z], 0, 1)\n",
    "            \n",
    "            for z in zom:\n",
    "                precision[z] = np.insert(precision[z], 0, 0)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "            \n",
    "            ax.set_ylabel('Precision')\n",
    "            ax.set_xlabel('Recall')\n",
    "            ax.set_title(test_datasets_names[curr_test_set_index])\n",
    "\n",
    "            ax.plot(recall[\"micro\"], precision[\"micro\"], label='Micro-average (area = {0:0.2f})'''.format(average_precision[\"micro\"]))\n",
    "\n",
    "            for i in range(2):\n",
    "                ax.plot(recall[i], precision[i], label=f'{classes[i]} (area = {average_precision[i]:.2f})')\n",
    "\n",
    "            ax = helper.init_precision_recall_plots(fig, ax, name = test_datasets_names[curr_test_set_index], save = True)\n",
    "\n",
    "        cm = confusion_matrix(y_test, ensemble_preds)\n",
    "        if cm.shape != (2, 2):\n",
    "            case_or_control_num = cm[0,0]\n",
    "            cm = [[0,0],[0,0]]\n",
    "            if y_test[1] == 1 and ensemble_preds[1] == 1:\n",
    "                cm = [[0,0],[0,case_or_control_num]]\n",
    "            else:\n",
    "                cm = [[case_or_control_num,0],[0,0]]\n",
    "        \n",
    "        cm = np.array(cm)\n",
    "\n",
    "        TN = cm[0,0]\n",
    "        FP = cm[0,1]\n",
    "        FN = cm[1,0]\n",
    "        TP = cm[1,1]\n",
    "\n",
    "        if TP+FN!=0 and TN+FP==0:\n",
    "            n_genes_recall.append(TP/(TP+FN))     \n",
    "\n",
    "        if TN+FP!=0 and TP+FN==0:\n",
    "            n_genes_specificity.append(TN/(TN+FP))\n",
    "        \n",
    "        if n == _USE_FIRST_N_GENES_CONF:\n",
    "            confusions.append(cm.copy())\n",
    "            \n",
    "    if n_genes_recall:\n",
    "        recall_scores.append(n_genes_recall.copy())\n",
    "    \n",
    "    if n_genes_specificity:\n",
    "        specificity_scores.append(n_genes_specificity.copy())\n",
    "\n",
    "    f1_scores.append(n_genes_f1.copy())\n",
    "    accuracies.append(n_genes_accs.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_counter = 0\n",
    "specificity_counter = 0\n",
    "\n",
    "for i in range(len(test_datasets_names)):\n",
    "    \n",
    "    x = np.arange(len(n_genes))\n",
    "    cm = confusions[i]\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "\n",
    "    if cm[0,0] == 0 and cm[0,1] == 0:\n",
    "        y = np.array(recall_scores)[:,recall_counter]\n",
    "        recall_counter = recall_counter + 1\n",
    "        ax.plot(x, y, label='Recall', marker='o', markerfacecolor='w', color='tab:purple')\n",
    "        ax.set_ylabel('Recall')\n",
    "\n",
    "    elif cm[1,0] == 0 and cm[1,1] == 0:\n",
    "        y = np.array(specificity_scores)[:,specificity_counter]\n",
    "        specificity_counter = specificity_counter + 1\n",
    "        ax.plot(x, y, label='Specificity', marker='o', markerfacecolor='w', color='tab:green')\n",
    "        ax.set_ylabel('Specificity') \n",
    "\n",
    "    elif cm[0,0]+cm[0,1] == cm[1,0]+cm[1,1]:\n",
    "        y = np.array(accuracies)[:,i]\n",
    "        ax.plot(x, y, label='Accuracy', marker='o', markerfacecolor='w', color='tab:blue')\n",
    "        ax.set_ylabel('Accuracy') \n",
    "\n",
    "    else:\n",
    "        y = np.array(accuracies)[:,i]\n",
    "        ax.plot(x, y, label='Accuracy', marker='o', markerfacecolor='w', color='tab:blue')\n",
    "        y = np.array(f1_scores)[:,i]\n",
    "        ax.plot(x, y, label='F1-Score', marker='o', markerfacecolor='w', color='tab:orange')\n",
    "        ax.set_ylabel('Score')\n",
    "\n",
    "    ax.set_xlabel('# of Genes') \n",
    "    ax.set_title(test_datasets_names[i])\n",
    "    ax.set_xticklabels([1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "    ax.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "    ax.set_yticklabels([0.0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    ax.set_xlim(-0.5, 10.5)\n",
    "    ax.set_ylim(0.0, 1.05)\n",
    "    ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "    fig.savefig('Pictures/'+test_datasets_names[i]+'.png', format='png', dpi=500)\n",
    "    helper.plot_confusion_matrix(_USE_FIRST_N_GENES_CONF, confusions[i], class_names= ['Control','Case'], name=test_datasets_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls_diagnosed_as_case = 0\n",
    "total_controls = 0\n",
    "cases_diagnosed_as_control = 0\n",
    "total_cases = 0\n",
    "\n",
    "for cm in confusions:\n",
    "    controls_diagnosed_as_case = controls_diagnosed_as_case + cm[0,1]\n",
    "    total_controls = total_controls + cm[0,0] + cm[0,1]\n",
    "    cases_diagnosed_as_control = cases_diagnosed_as_control + cm[1,0]\n",
    "    total_cases = total_cases + cm[1,0] + cm[1,1]\n",
    "\n",
    "\n",
    "print(f'Total Cases: {total_cases}')\n",
    "print(f'Cases misdiagnosed as controls: {cases_diagnosed_as_control}')\n",
    "print(f'Total Controls: {total_controls}')\n",
    "print(f'Controls misdiagnosed as controls: {controls_diagnosed_as_case}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99aada5f0ddb21c3452878f90434fad4cf2f4ffe5b678b27461e3cd3287cc503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
